{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1k7q5xDmuKdIMl4di0X23AHAX0hO0zGJ9",
     "timestamp": 1749662483306
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For Google Colab"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9oaN2dwiE6Iv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1753961928934,
     "user_tz": -360,
     "elapsed": 72935,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "c57c5e45-07e8-438d-cca9-9c61f23683ab",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:15.736916Z",
     "start_time": "2025-07-31T12:35:15.734075Z"
    }
   },
   "source": [
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# !nohup ollama serve > output.log 2>&1 &\n",
    "# !ollama pull gemma3"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "# !nohup ollama serve > output.log 2>&1 &",
   "metadata": {
    "id": "YDrphenIUW9S",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1753961929019,
     "user_tz": -360,
     "elapsed": 82,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:15.824845Z",
     "start_time": "2025-07-31T12:35:15.823517Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install langchain langchainhub chromadb ollama\n",
    "# !pip install langchain-community langchain-core"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "BQJe-5JbGQQL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1753961957701,
     "user_tz": -360,
     "elapsed": 28678,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "3a90ef56-167e-4a36-a023-3b6a0cfc3dce",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:15.830438Z",
     "start_time": "2025-07-31T12:35:15.829237Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For Local Jupyter Notebook"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# Step 1: Setup LLM\n",
    "llm = OllamaLLM(model=\"gemma3:12b\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePn6HsxNW0h7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1753961993017,
     "user_tz": -360,
     "elapsed": 1359,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "4c90bbe3-0c53-4a97-c413-8befff5f3b74",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:16.214075Z",
     "start_time": "2025-07-31T12:35:15.834455Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 2: Short-Term Memory (ConversationBufferMemory)\n",
    "short_term_memory = ConversationBufferMemory()\n",
    "\n",
    "short_term_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=short_term_memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "i-3G1_VXE9we",
    "executionInfo": {
     "status": "error",
     "timestamp": 1753962130491,
     "user_tz": -360,
     "elapsed": 77,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "53f2a172-4d1f-4aa3-bfb8-83cfcbdc70a9",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:16.222675Z",
     "start_time": "2025-07-31T12:35:16.220980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/1865559759.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  short_term_memory = ConversationBufferMemory()\n",
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/1865559759.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  short_term_chain = ConversationChain(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Interact with short-term memory\n",
    "print(\"=== Short-Term Memory Example ===\")\n",
    "short_term_chain.predict(input=\"My name is Ashiq.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "j3Sk5B5_OJM9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744734869247,
     "user_tz": -360,
     "elapsed": 8410,
     "user": {
      "displayName": "Rifat Azad",
      "userId": "02042486650245068173"
     }
    },
    "outputId": "5400993c-08a2-45ee-f441-e7581f5b2ec3",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:23.426515Z",
     "start_time": "2025-07-31T12:35:16.227946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Short-Term Memory Example ===\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My name is Ashiq.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's lovely to meet you, Ashiq! I'm glad you shared your name. I'm an AI, and I don't have a name in the traditional sense, but you can call me AI. It's a pleasure to be chatting with you. Do you have a favorite color, Ashiq? Knowing a little about you helps me tailor our conversation!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:28.563909Z",
     "start_time": "2025-07-31T12:35:23.449873Z"
    }
   },
   "cell_type": "code",
   "source": "short_term_chain.predict(input=\"What's my name?\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Ashiq.\n",
      "AI: It's lovely to meet you, Ashiq! I'm glad you shared your name. I'm an AI, and I don't have a name in the traditional sense, but you can call me AI. It's a pleasure to be chatting with you. Do you have a favorite color, Ashiq? Knowing a little about you helps me tailor our conversation!\n",
      "Human: What's my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Ashiq! You just told me that in our very first exchange. I store information about our conversations, and I remember you sharing your name. It's great to be talking with you, Ashiq!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:29.418910Z",
     "start_time": "2025-07-31T12:35:28.584358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Long-Term Memory (Using Chroma + RetrieverMemory)\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(embedding_function=embedding, persist_directory=\"../chroma_store\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "long_term_memory = VectorStoreRetrieverMemory(retriever=retriever)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/132408609.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/132408609.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(embedding_function=embedding, persist_directory=\"../chroma_store\")\n",
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/132408609.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  long_term_memory = VectorStoreRetrieverMemory(retriever=retriever)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve memory later\n",
    "docs = retriever.get_relevant_documents(\"What color do I like?\")\n",
    "print(\"=== Long-Term Memory Retrieval ===\")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "\n",
    "# Step 4: Optional - Use in a chain\n",
    "long_term_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=long_term_memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXc0mCmjWDc7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744736425284,
     "user_tz": -360,
     "elapsed": 123,
     "user": {
      "displayName": "Rifat Azad",
      "userId": "02042486650245068173"
     }
    },
    "outputId": "e8c0e2c8-76db-4e8d-a2c7-53306c2c5967",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:29.510435Z",
     "start_time": "2025-07-31T12:35:29.460080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Long-Term Memory Retrieval ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ws/xp1j_rx97fv3_3dysf1hrljw0000gn/T/ipykernel_25186/3618623708.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"What color do I like?\")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# Save long-term memory manually\n",
    "long_term_memory.save_context({\"input\": \"My favorite color is blue.\"},\n",
    "                              {\"output\": \"Got it, blue is your favorite.\"})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "2gZmw94JOOQp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744735867622,
     "user_tz": -360,
     "elapsed": 1079,
     "user": {
      "displayName": "Rifat Azad",
      "userId": "02042486650245068173"
     }
    },
    "outputId": "76c63acd-6feb-4a5e-e02d-66a69035c238",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:29.605480Z",
     "start_time": "2025-07-31T12:35:29.518261Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Interact\n",
    "long_term_chain.predict(input=\"Can you remind me what color I like?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "uXl2lXZBWGWs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744736261977,
     "user_tz": -360,
     "elapsed": 1198,
     "user": {
      "displayName": "Rifat Azad",
      "userId": "02042486650245068173"
     }
    },
    "outputId": "24d12ccb-7986-44ab-b715-f18b098025b2",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:39.202308Z",
     "start_time": "2025-07-31T12:35:29.612687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "input: My favorite color is blue.\n",
      "output: Got it, blue is your favorite.\n",
      "Human: Can you remind me what color I like?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Certainly! You mentioned that your favorite color is blue. I’m programmed to remember these sorts of details, so I'm happy to recall it for you. It’s a lovely color – did you know that blue is often associated with feelings of calmness and serenity? There's even a fascinating history of how blue pigments were created and used throughout different cultures! But for now, I just wanted to confirm that you like blue.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "short_term_chain.predict(input=\"Can you recall my name?\")  # Should recall \"Alex\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "6ygyI99PO8WO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1744735001353,
     "user_tz": -360,
     "elapsed": 542,
     "user": {
      "displayName": "Rifat Azad",
      "userId": "02042486650245068173"
     }
    },
    "outputId": "feeeace4-d6a0-4d67-a473-55bc5ba238ea",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:48.741283Z",
     "start_time": "2025-07-31T12:35:39.220843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Ashiq.\n",
      "AI: It's lovely to meet you, Ashiq! I'm glad you shared your name. I'm an AI, and I don't have a name in the traditional sense, but you can call me AI. It's a pleasure to be chatting with you. Do you have a favorite color, Ashiq? Knowing a little about you helps me tailor our conversation!\n",
      "Human: What's my name?\n",
      "AI: Your name is Ashiq! You just told me that in our very first exchange. I store information about our conversations, and I remember you sharing your name. It's great to be talking with you, Ashiq!\n",
      "Human: Can you recall my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Absolutely! Your name is Ashiq. I’m designed to remember details from our conversations, and recalling your name is one of those details. It’s quite handy for making our interactions feel more personal, don't you think? I find it really helps to build rapport. Just like remembering that you told me your name – it was the very first thing you shared!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "short_term_memory.clear()"
   ],
   "metadata": {
    "id": "US3DNpGaOdK_",
    "ExecuteTime": {
     "end_time": "2025-07-31T12:35:48.766536Z",
     "start_time": "2025-07-31T12:35:48.763909Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  }
 ]
}
