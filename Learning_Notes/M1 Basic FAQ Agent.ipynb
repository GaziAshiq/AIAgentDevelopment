{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1QF6AWpLNm74f7BatSOP5oHh-qK-LGPDa",
     "timestamp": 1742923472254
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install openai langchain langchain-community langchain-core python-dotenv"
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Sample FAQ data on agents and their benefits\n",
    "faq_data = [\n",
    "    {\"question\": \"What is an AI agent?\", \"answer\": \"An AI agent is a system that can autonomously perform tasks based on inputs and learned knowledge.\"},\n",
    "    {\"question\": \"How can AI agents improve efficiency?\", \"answer\": \"AI agents can automate repetitive tasks, streamline decision-making, and enhance customer support with accurate responses.\"},\n",
    "    {\"question\": \"What industries benefit most from AI agents?\", \"answer\": \"Industries like healthcare, e-commerce, finance, and customer support heavily benefit from AI agents.\"},\n",
    "    {\"question\": \"Are AI agents customizable?\", \"answer\": \"Yes, AI agents can be trained with specific data and prompts to suit various use cases.\"}\n",
    "]\n",
    "\n",
    "# Initialize the GPT-4o mini model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Create a template for chatbot responses\n",
    "faq_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"You are an AI chatbot. Answer this question based on the provided FAQ data:\\n{faq_data}\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Create a chain for handling chatbot interactions\n",
    "faq_chain = LLMChain(\n",
    "    prompt=faq_template,\n",
    "    llm=chat_model\n",
    ")\n",
    "\n",
    "# Function to handle user queries\n",
    "def chatbot():\n",
    "    print(\"Welcome to the AI FAQ Chatbot! Ask me about agents and their benefits. Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        response = faq_chain.run({\"question\": user_input, \"faq_data\": faq_data})\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Run the chatbot in Colab\n",
    "display(chatbot())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "HgE7tjDgla-U",
    "executionInfo": {
     "status": "error",
     "timestamp": 1742944286981,
     "user_tz": -360,
     "elapsed": 6667,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "15130c1e-859a-47cd-d4dc-f1cf2479694b"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI FAQ Chatbot! Ask me about agents and their benefits. Type 'exit' to stop.\n",
      "You: hi\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-26db2503b0e2>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;31m# Run the chatbot in Colab\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchatbot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-26db2503b0e2>\u001B[0m in \u001B[0;36mchatbot\u001B[0;34m()\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Goodbye!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfaq_chain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"question\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0muser_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"faq_data\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfaq_data\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Bot:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001B[0m in \u001B[0;36mwarning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m                 \u001B[0mwarned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m                 \u001B[0memit_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 181\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32masync\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mawarning_emitting_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    604\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    605\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"`run` supports only one positional argument.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 606\u001B[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[0m\u001B[1;32m    607\u001B[0m                 \u001B[0m_output_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    608\u001B[0m             ]\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001B[0m in \u001B[0;36mwarning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m                 \u001B[0mwarned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m                 \u001B[0memit_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 181\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32masync\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mawarning_emitting_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    387\u001B[0m         }\n\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m         return self.invoke(\n\u001B[0m\u001B[1;32m    390\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m             \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mRunnableConfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36minvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 170\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    171\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36minvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    158\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    159\u001B[0m             outputs = (\n\u001B[0;32m--> 160\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    161\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCallbackManagerForChainRun\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m     ) -> Dict[str, str]:\n\u001B[0;32m--> 126\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    127\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_outputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mrun_manager\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mllm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseLanguageModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m             return self.llm.generate_prompt(\n\u001B[0m\u001B[1;32m    139\u001B[0m                 \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m                 \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36mgenerate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    841\u001B[0m     ) -> LLMResult:\n\u001B[1;32m    842\u001B[0m         \u001B[0mprompt_messages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_messages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 843\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_messages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    844\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m     async def agenerate_prompt(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    682\u001B[0m                 results.append(\n\u001B[0;32m--> 683\u001B[0;31m                     self._generate_with_cache(\n\u001B[0m\u001B[1;32m    684\u001B[0m                         \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    685\u001B[0m                         \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36m_generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    906\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_generate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_manager\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 908\u001B[0;31m                 result = self._generate(\n\u001B[0m\u001B[1;32m    909\u001B[0m                     \u001B[0mmessages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    910\u001B[0m                 )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001B[0m in \u001B[0;36m_generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    474\u001B[0m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    475\u001B[0m         }\n\u001B[0;32m--> 476\u001B[0;31m         response = self.completion_with_retry(\n\u001B[0m\u001B[1;32m    477\u001B[0m             \u001B[0mmessages\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmessage_dicts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    478\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001B[0m in \u001B[0;36mcompletion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_openai_v1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m         \u001B[0mretry_decorator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_retry_decorator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m                         \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    278\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 279\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m  \u001B[0;31m# type: ignore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    912\u001B[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[1;32m    913\u001B[0m         \u001B[0mvalidate_response_format\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse_format\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 914\u001B[0;31m         return self._post(\n\u001B[0m\u001B[1;32m    915\u001B[0m             \u001B[0;34m\"/chat/completions\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m             body=maybe_transform(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mpost\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1240\u001B[0m             \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"post\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mto_httpx_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1241\u001B[0m         )\n\u001B[0;32m-> 1242\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mResponseT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream_cls\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream_cls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1244\u001B[0m     def patch(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    917\u001B[0m             \u001B[0mretries_taken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 919\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m    920\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    921\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1006\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mremaining_retries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1008\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1009\u001B[0m                     \u001B[0minput_options\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1010\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1055\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1058\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1059\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1006\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mremaining_retries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1008\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1009\u001B[0m                     \u001B[0minput_options\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1010\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1055\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1058\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1059\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1021\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1022\u001B[0m             \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Re-raising status error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1023\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_status_error_from_response\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1024\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1025\u001B[0m         return self._process_response(\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AI FAQ Chatbot Code Explanation\n",
    "![Code explanation in flow chart](https://lh3.googleusercontent.com/fife/ALs6j_EDrqGwgruo2lGaqFYMCcp495dVJbKvHpHhdGjGkA35ZOochkI6TAI6_1h23CVh06hVz4nKeI-KCYYrC8yXZSczAPeHhSWQfHMbnjySar_678_V_kw5g15yP7RZh_JiePWdX0rPqNifX7GzbjAGyKDMpx1XloX4e3tu8n-bblpf7IJOsJrCqaOr-5IL6k8zGNJ8wByachzpTXn1DQI2W9dGH_Y53R2Jxrgb8Hk1KxPIhHojJ58LVpqPDz_G37b6E_Gp-ZQXpxCRnqJI_aU8T1GrAK5g6P4Kce0vWUoBjd6jMr3M680wTNnc60rPnoV40WUp9SsNUPEzbgam6-Zvdtm9tIvCxjpwIqw0k_XGkE5-9fw_P8TEJzoKsQRF76gbGrNbMv2JVZqiO_0sfMSFH_sDMWolnVfvawxjOl-IawslgHC_vzp4RCFnR8jnTvK7gjaUU9KbAlIegprgJXl_K_ANUblSQ53f0f1H5kORrzdxt-vLG-i5c0ODLzzdVpr7TRheEWYhww26iHtNAkEIeT1gant-molFbOyRGmvofiAw5-4nltfMpeDAymEoKy2iPLIiWP3ZQylN907RRZ2QNhJNrBPrlV3o9qXf8bdkJCTwcr1rwa60N4ZOKgK5LsKzorkOyof6x3yCsJLCnA82zec-_4i3dDTNU5wuOPeVv1iSvTYETTBoUeOmZrPMsmNawL9yTa9PYBaTmfCU7fhsV1A2JAv97ZpEAABXNNKSNu1474_O8N-s-XZh9tHOcnwMjZWYWxmHMRyow0mJVVMvl5pwDlkadX02KZ-xYKRk1B4pWDm1PQolpc9POBfW0RT6lxyYSxP65-Pg-xyMTEspWyyo3T9gSMQyDaSGZNqMCFp6XhplinnG758ILL-SyzfaQM9jvT_RkQh5YAyZI2uiakOC90KzBJUseRjxdzfyrx9-w-WEGLsGghPa02AeZqobsnkPjnnDGMpInrv1Zz9xWsLRQTcU2yeUj7vRUOvroCFHyONTsdL6E6tJrMFxmT4berLy9AXBhQo3GJwq0YaHML70-3kf3ZHROlh_2UU4JMQoBBVcwo9sdLOLf9AonnPY8LtZ8-d02s98cyHDH2tDthOvGSh9aJ52xFfqlWOyHBsda7J-1R1qHc3wYS04GoGhVESm74o0WNT1K0I5xkb0C7AHX_vQurWB4f_VSU140np1eRpePpl0SdZ9a5xXar22zRyG71PW1o6BFu2gnx79zWqDAq63tKpVTLLw0Lc6lUT7Ed5cFaZr-caXcwOfLt-LMfd0wwWR1TQW0I6GMPcIPgX0P3b0ugkvW9ZYlKvBhH4N003zrDPXu1WZU5PmSNUHtO0CdZKip8RbCT79iPkJ86NrdZ0IizkLChZGTTE83lMLRvRoOXEx7whpVTUi50zDOA9yCXfdh0l8gR8ZessV-qO9Jpg-1lMjxoI9t2gk8P-8yw8kZKhMizL-wW6Q2VWHiLY11arR-y8b7M1e_gw5lE573lyANk_kLYBU15kdPk-UCPTE2Yt0WcLIkzivO4WOr651_kKYjRHHy9fTPnEebhWanBjXoERppF2ngVoGS9M_oMQuZ0nNvyLfeYoah5iLVX-MzPWYBEZ0x2QyzOHeBQ=w2880-h1631)\n",
    "\n",
    "## Step 1: Install Required Libraries\n",
    "- The code installs three key libraries:\n",
    "  - `openai`: For interacting with the OpenAI API.\n",
    "  - `langchain`: To simplify the creation of LLM chains for structured responses.\n",
    "  - `graphviz`: For visualizing the chatbot's flow using diagrams.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Import Necessary Libraries\n",
    "- The required libraries are imported:\n",
    "  - `openai` for API interaction.\n",
    "  - `ChatOpenAI`, `PromptTemplate`, and `LLMChain` from LangChain to build the chatbot's logic.\n",
    "  - `Digraph` from Graphviz for generating the flowchart visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Set Up OpenAI API Key\n",
    "- The OpenAI API key is set using the `os` environment variable.\n",
    "- This ensures secure access to GPT-4o for generating chatbot responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Define Sample FAQ Data\n",
    "- A sample dataset is defined as a list of dictionaries, each containing:\n",
    "  - **`question`**: The question asked.\n",
    "  - **`answer`**: The corresponding answer.\n",
    "- This dataset helps guide the chatbot's responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Initialize the gpt-4o-mini Model\n",
    "- The GPT-4o model is initialized with:\n",
    "  - **`model=\"gpt-4o-mini\"`**: Uses GPT-4o for optimal performance.\n",
    "  - **`temperature=0.7`**: Controls randomness — higher values increase creativity, while lower values make responses more focused.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Create a Prompt Template\n",
    "- The prompt template structures the conversation by:\n",
    "  - Incorporating the FAQ data.\n",
    "  - Formulating questions in a clear format for the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Create Chain for Chatbot\n",
    "- An `LLMChain` is created to:\n",
    "  - Process user queries using the chatbot model.\n",
    "  - Combine the template and the GPT-4o model.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Define Chatbot Function\n",
    "- The chatbot function handles the user interaction loop:\n",
    "  - It greets the user.\n",
    "  - Continuously accepts user input until 'exit' is typed.\n",
    "  - It fetches responses from the `faq_chain` using the user's question.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9: Define Flowchart Generation\n",
    "- The `generate_flowchart` function creates a visual diagram of the code’s flow:\n",
    "  - Nodes represent steps in the code.\n",
    "  - Edges connect these nodes to depict the logical flow.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 10: Run Chatbot and Flowchart\n",
    "- The chatbot function is executed to start user interaction.\n",
    "- The `generate_flowchart()` function produces a `.png` visualization of the chatbot’s logic.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Notes\n",
    "- The chatbot effectively leverages GPT-4o for natural language responses.\n",
    "- The flowchart provides a clear visual guide to the code’s structure.\n"
   ],
   "metadata": {
    "id": "yQL6QP17rStG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Sample FAQ data on agents and their benefits\n",
    "faq_data = [\n",
    "    {\"question\": \"What is an AI agent?\", \"answer\": \"An AI agent is a system that can autonomously perform tasks based on inputs and learned knowledge.\"},\n",
    "    {\"question\": \"How can AI agents improve efficiency?\", \"answer\": \"AI agents can automate repetitive tasks, streamline decision-making, and enhance customer support with accurate responses.\"},\n",
    "    {\"question\": \"What industries benefit most from AI agents?\", \"answer\": \"Industries like healthcare, e-commerce, finance, and customer support heavily benefit from AI agents.\"},\n",
    "    {\"question\": \"Are AI agents customizable?\", \"answer\": \"Yes, AI agents can be trained with specific data and prompts to suit various use cases.\"}\n",
    "]\n",
    "\n",
    "# Initialize the GPT-4o mini model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Create a template for chatbot responses\n",
    "faq_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"You are an AI chatbot. Answer this question based on the provided FAQ data:\\n{faq_data}\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Create a chain for handling chatbot interactions\n",
    "faq_chain = LLMChain(\n",
    "    prompt=faq_template,\n",
    "    llm=chat_model\n",
    ")\n",
    "\n",
    "# Function to handle user queries\n",
    "def chatbot():\n",
    "    print(\"Welcome to the AI FAQ Chatbot! Ask me about agents and their benefits. Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        response = faq_chain.run({\"question\": user_input, \"faq_data\": faq_data})\n",
    "        print(\"Bot:\", response)\n",
    "        # Add a delay to avoid hitting the rate limit\n",
    "        time.sleep(3) # Wait for 3 seconds\n",
    "\n",
    "# Run the chatbot in Colab\n",
    "from IPython.display import display\n",
    "display(chatbot())"
   ],
   "metadata": {
    "id": "SB61PFNRrTaL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1742944430106,
     "user_tz": -360,
     "elapsed": 5644,
     "user": {
      "displayName": "Gazi Ashiq",
      "userId": "02384138412179728115"
     }
    },
    "outputId": "622195c1-77e9-4511-a177-b433974fb7e8"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI FAQ Chatbot! Ask me about agents and their benefits. Type 'exit' to stop.\n",
      "You: hi\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-fb6be88a5107>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;31m# Run the chatbot in Colab\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mIPython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchatbot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-6-fb6be88a5107>\u001B[0m in \u001B[0;36mchatbot\u001B[0;34m()\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Goodbye!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfaq_chain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"question\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0muser_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"faq_data\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfaq_data\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Bot:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;31m# Add a delay to avoid hitting the rate limit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001B[0m in \u001B[0;36mwarning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m                 \u001B[0mwarned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m                 \u001B[0memit_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 181\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32masync\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mawarning_emitting_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    604\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    605\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"`run` supports only one positional argument.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 606\u001B[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[0m\u001B[1;32m    607\u001B[0m                 \u001B[0m_output_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    608\u001B[0m             ]\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001B[0m in \u001B[0;36mwarning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m                 \u001B[0mwarned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m                 \u001B[0memit_warning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 181\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;32masync\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mawarning_emitting_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    387\u001B[0m         }\n\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m         return self.invoke(\n\u001B[0m\u001B[1;32m    390\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m             \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mRunnableConfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36minvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 170\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    171\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36minvoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    158\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    159\u001B[0m             outputs = (\n\u001B[0;32m--> 160\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    161\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCallbackManagerForChainRun\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m     ) -> Dict[str, str]:\n\u001B[0;32m--> 126\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    127\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_outputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mrun_manager\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mllm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseLanguageModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m             return self.llm.generate_prompt(\n\u001B[0m\u001B[1;32m    139\u001B[0m                 \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m                 \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36mgenerate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    841\u001B[0m     ) -> LLMResult:\n\u001B[1;32m    842\u001B[0m         \u001B[0mprompt_messages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_messages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 843\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_messages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    844\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m     async def agenerate_prompt(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    682\u001B[0m                 results.append(\n\u001B[0;32m--> 683\u001B[0;31m                     self._generate_with_cache(\n\u001B[0m\u001B[1;32m    684\u001B[0m                         \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    685\u001B[0m                         \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001B[0m in \u001B[0;36m_generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    906\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_generate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_manager\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 908\u001B[0;31m                 result = self._generate(\n\u001B[0m\u001B[1;32m    909\u001B[0m                     \u001B[0mmessages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    910\u001B[0m                 )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001B[0m in \u001B[0;36m_generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    474\u001B[0m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    475\u001B[0m         }\n\u001B[0;32m--> 476\u001B[0;31m         response = self.completion_with_retry(\n\u001B[0m\u001B[1;32m    477\u001B[0m             \u001B[0mmessages\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmessage_dicts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    478\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001B[0m in \u001B[0;36mcompletion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_openai_v1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m         \u001B[0mretry_decorator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_retry_decorator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m                         \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    278\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 279\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m  \u001B[0;31m# type: ignore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001B[0m in \u001B[0;36mcreate\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    912\u001B[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[1;32m    913\u001B[0m         \u001B[0mvalidate_response_format\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse_format\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 914\u001B[0;31m         return self._post(\n\u001B[0m\u001B[1;32m    915\u001B[0m             \u001B[0;34m\"/chat/completions\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    916\u001B[0m             body=maybe_transform(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mpost\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1240\u001B[0m             \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"post\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mto_httpx_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1241\u001B[0m         )\n\u001B[0;32m-> 1242\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mResponseT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream_cls\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream_cls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1243\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1244\u001B[0m     def patch(\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    917\u001B[0m             \u001B[0mretries_taken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 919\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m    920\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    921\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1006\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mremaining_retries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1008\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1009\u001B[0m                     \u001B[0minput_options\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1010\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1055\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1058\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1059\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1006\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mremaining_retries\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_should_retry\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1007\u001B[0m                 \u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1008\u001B[0;31m                 return self._retry_request(\n\u001B[0m\u001B[1;32m   1009\u001B[0m                     \u001B[0minput_options\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1010\u001B[0m                     \u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1055\u001B[0m         \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m         return self._request(\n\u001B[0m\u001B[1;32m   1058\u001B[0m             \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1059\u001B[0m             \u001B[0mcast_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcast_to\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001B[0m in \u001B[0;36m_request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1021\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1022\u001B[0m             \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Re-raising status error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1023\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_status_error_from_response\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1024\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1025\u001B[0m         return self._process_response(\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ]
  }
 ]
}
